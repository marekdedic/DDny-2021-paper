\section{Introduction}

\todo{R1: It is very difficult to understand the concrete goal of the paper. It is important to describe the added value of the proposed framework comparing with the existing HARP method.}
\todo{R1: What is the motivation behind using partially injective homomorphism instead of existing coarsening techniques of HARP? is it simpler to compute? does it improve the learning performance? it is important that the paper provides a clear answer to this question, as this point is one of its main contributions.}
Graph-based models for machine learning are often used for task with millions of nodes and billions of edges. For such tasks, many machine learning algorithms may be computationally intractable. As a way to combat these growing demands, we look at the trade-off between performance, complexity and method generality. Our work in progress builds on HARP \cite{chen_harp_2018}, a method for pretraining graph-based learning algorithms by first learning on scaled down versions of the graph in question. While in HARP, this downscaling of the graphs is done in a fixed way, we propose a formal generalization of such a downscaling using partially injective homomorphisms, a theoretical result with background in data mining. The performance characteristics of HARP are also studied in our work, with the focus being on comparing the performance of models with lower training costs, potentially enabling the use of graph-based algorithms on bigger datasets than previously possible.

In the next section, the graph learning algorithm HARP is presented, followed by a section presenting partially injective homomorphisms. Section \ref{sec:harp-as-pihom} connects these ideas and Section \ref{sec:performance-vs-complexity} discusses the performance characteristics of HARP. Finally, Section \ref{sec:experiments} supports our theses with experimental evaluation.
